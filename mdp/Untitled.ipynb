{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38eb0801-f704-4331-b009-e58797c58ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium.envs.toy_text.cliffwalking import CliffWalkingEnv\n",
    "from gymnasium.error import DependencyNotInstalled\n",
    "from os import path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def update_values(enviroment, env, policy, gamma, threshold=0.0001):\n",
    "    values = [0 for i in range(env.observation_space.n)]\n",
    "    q_values = {}\n",
    "    for state in range(48):\n",
    "        q_values[state] = {}\n",
    "        for ac in possible_actions(state):\n",
    "            q_values[state][ac] = 0\n",
    "\n",
    "    cliff = [u[0] * 12 + u[1] for u in env.cliff_positions]\n",
    "    t = 0\n",
    "    while (t < 1000):\n",
    "        distance = 0\n",
    "        last_val = values.copy()\n",
    "        for state in range(env.observation_space.n):\n",
    "            if (state == 47):\n",
    "                mx_q = 4000\n",
    "            elif (state in cliff):\n",
    "                mx_q = -100\n",
    "            else:\n",
    "                for action in possible_actions(state):\n",
    "                    q_values[state][action] = 0\n",
    "                    sum = 0\n",
    "                    for probability, reward, nxt_state in enviroment[state][action]:\n",
    "                        q_values[state][action] += probability * (reward + gamma * values[nxt_state])\n",
    "                    mx_q = q_values[state][policy[state]]\n",
    "\n",
    "            values[state] = mx_q\n",
    "\n",
    "        t += 1\n",
    "\n",
    "    return q_values, values\n",
    "\n",
    "def enviroment_init():\n",
    "    enviroment = {}\n",
    "    for state in range(47):\n",
    "        c = state % 12\n",
    "        enviroment[state] = {}\n",
    "        for action in possible_actions(state):\n",
    "            enviroment[state][action] = []\n",
    "            h = actions(state, action)\n",
    "            for i in range(len(h)):\n",
    "                reward = (-1 / (10 * (c + 1))) ** 3\n",
    "                enviroment[state][action].append((1 / 3, reward, next_state(state, h[i])))\n",
    "\n",
    "    return enviroment\n",
    "\n",
    "\n",
    "def mdp_algorithm(enviroment, env, gamma=0.9):\n",
    "    values = [0 for i in range(env.observation_space.n)]\n",
    "    policy = [possible_actions(i)[0] for i in range(48)]\n",
    "    t = 1\n",
    "    while (t < 100):\n",
    "        q_values, values = update_values(enviroment, env, policy, gamma)\n",
    "        changed = False\n",
    "\n",
    "        for state in range(env.observation_space.n):\n",
    "            mxId = possible_actions(state)[0]\n",
    "            for action in possible_actions(state):\n",
    "                if q_values[state][action] > q_values[state][mxId]:\n",
    "                    mxId = action\n",
    "            if policy[state] != mxId:\n",
    "                policy[state] = mxId\n",
    "                changed = True\n",
    "\n",
    "        if not changed:\n",
    "            break\n",
    "\n",
    "        t += 1\n",
    "    return values, policy\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd9bacc-e509-4395-93ec-b8776ac028a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change this class\n",
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3\n",
    "image_path = path.join(path.dirname(gym.__file__), \"envs\", \"toy_text\")\n",
    "\n",
    "class CliffWalking(CliffWalkingEnv):\n",
    "    def __init__(self, is_hardmode=True, num_cliffs=10, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.is_hardmode = is_hardmode\n",
    "\n",
    "        # Generate random cliff positions\n",
    "        if self.is_hardmode:\n",
    "            self.num_cliffs = num_cliffs\n",
    "            self._cliff = np.zeros(self.shape, dtype=bool)\n",
    "            self.start_state = (3, 0)\n",
    "            self.terminal_state = (self.shape[0] - 1, self.shape[1] - 1)\n",
    "            self.cliff_positions = []\n",
    "            while len(self.cliff_positions) < self.num_cliffs:\n",
    "                new_row = np.random.randint(0, 4)\n",
    "                new_col = np.random.randint(0, 11)\n",
    "                state = (new_row, new_col)\n",
    "                if (\n",
    "                    (state not in self.cliff_positions)\n",
    "                    and (state != self.start_state)\n",
    "                    and (state != self.terminal_state)\n",
    "                ):\n",
    "                    self._cliff[new_row, new_col] = True\n",
    "                    if not self.is_valid():\n",
    "                        self._cliff[new_row, new_col] = False\n",
    "                        continue\n",
    "                    self.cliff_positions.append(state)\n",
    "\n",
    "        # Calculate transition probabilities and rewards\n",
    "        self.P = {}\n",
    "        for s in range(self.nS):\n",
    "            position = np.unravel_index(s, self.shape)\n",
    "            self.P[s] = {a: [] for a in range(self.nA)}\n",
    "            self.P[s][UP] = self._calculate_transition_prob(position, [-1, 0])\n",
    "            self.P[s][RIGHT] = self._calculate_transition_prob(position, [0, 1])\n",
    "            self.P[s][DOWN] = self._calculate_transition_prob(position, [1, 0])\n",
    "            self.P[s][LEFT] = self._calculate_transition_prob(position, [0, -1])\n",
    "\n",
    "    def _calculate_transition_prob(self, current, delta):\n",
    "        new_position = np.array(current) + np.array(delta)\n",
    "        new_position = self._limit_coordinates(new_position).astype(int)\n",
    "        new_state = np.ravel_multi_index(tuple(new_position), self.shape)\n",
    "        if self._cliff[tuple(new_position)]:\n",
    "            return [(1.0, self.start_state_index, -100, False)]\n",
    "\n",
    "        terminal_state = (self.shape[0] - 1, self.shape[1] - 1)\n",
    "        is_terminated = tuple(new_position) == terminal_state\n",
    "        return [(1 / 3, new_state, -1, is_terminated)]\n",
    "\n",
    "    # DFS to check that it's a valid path.\n",
    "    def is_valid(self):\n",
    "        frontier, discovered = [], set()\n",
    "        frontier.append((3, 0))\n",
    "        while frontier:\n",
    "            r, c = frontier.pop()\n",
    "            if not (r, c) in discovered:\n",
    "                discovered.add((r, c))\n",
    "                directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
    "                for x, y in directions:\n",
    "                    r_new = r + x\n",
    "                    c_new = c + y\n",
    "                    if r_new < 0 or r_new >= self.shape[0] or c_new < 0 or c_new >= self.shape[1]:\n",
    "                        continue\n",
    "                    if (r_new, c_new) == self.terminal_state:\n",
    "                        return True\n",
    "                    if not self._cliff[r_new][c_new]:\n",
    "                        frontier.append((r_new, c_new))\n",
    "        return False\n",
    "\n",
    "    def step(self, action):\n",
    "        if action not in [0, 1, 2, 3]:\n",
    "            raise ValueError(f\"Invalid action {action}   must be in [0, 1, 2, 3]\")\n",
    "\n",
    "        if self.is_hardmode:\n",
    "            match action:\n",
    "                case 0:\n",
    "                    action = np.random.choice([0, 1, 3], p=[1 / 3, 1 / 3, 1 / 3])\n",
    "                case 1:\n",
    "                    action = np.random.choice([0, 1, 2], p=[1 / 3, 1 / 3, 1 / 3])\n",
    "                case 2:\n",
    "                    action = np.random.choice([1, 2, 3], p=[1 / 3, 1 / 3, 1 / 3])\n",
    "                case 3:\n",
    "                    action = np.random.choice([0, 2, 3], p=[1 / 3, 1 / 3, 1 / 3])\n",
    "\n",
    "        return super().step(action)\n",
    "\n",
    "    def _render_gui(self, mode):\n",
    "        try:\n",
    "            import pygame\n",
    "        except ImportError as e:\n",
    "            raise DependencyNotInstalled(\n",
    "                \"pygame is not installed, run `pip install gymnasium[toy-text]`\"\n",
    "            ) from e\n",
    "        if self.window_surface is None:\n",
    "            pygame.init()\n",
    "\n",
    "            if mode == \"human\":\n",
    "                pygame.display.init()\n",
    "                pygame.display.set_caption(\"CliffWalking - Edited by Audrina & Kian\")\n",
    "                self.window_surface = pygame.display.set_mode(self.window_size)\n",
    "            else:  # rgb_array\n",
    "                self.window_surface = pygame.Surface(self.window_size)\n",
    "        if self.clock is None:\n",
    "            self.clock = pygame.time.Clock()\n",
    "        if self.elf_images is None:\n",
    "            hikers = [\n",
    "                path.join(image_path, \"img/elf_up.png\"),\n",
    "                path.join(image_path, \"img/elf_right.png\"),\n",
    "                path.join(image_path, \"img/elf_down.png\"),\n",
    "                path.join(image_path, \"img/elf_left.png\"),\n",
    "            ]\n",
    "            self.elf_images = [\n",
    "                pygame.transform.scale(pygame.image.load(f_name), self.cell_size)\n",
    "                for f_name in hikers\n",
    "            ]\n",
    "        if self.start_img is None:\n",
    "            file_name = path.join(image_path, \"img/stool.png\")\n",
    "            self.start_img = pygame.transform.scale(\n",
    "                pygame.image.load(file_name), self.cell_size\n",
    "            )\n",
    "        if self.goal_img is None:\n",
    "            file_name = path.join(image_path, \"img/cookie.png\")\n",
    "            self.goal_img = pygame.transform.scale(\n",
    "                pygame.image.load(file_name), self.cell_size\n",
    "            )\n",
    "        if self.mountain_bg_img is None:\n",
    "            bg_imgs = [\n",
    "                path.join(image_path, \"img/mountain_bg1.png\"),\n",
    "                path.join(image_path, \"img/mountain_bg2.png\"),\n",
    "            ]\n",
    "            self.mountain_bg_img = [\n",
    "                pygame.transform.scale(pygame.image.load(f_name), self.cell_size)\n",
    "                for f_name in bg_imgs\n",
    "            ]\n",
    "        if self.near_cliff_img is None:\n",
    "            near_cliff_imgs = [\n",
    "                path.join(image_path, \"img/mountain_near-cliff1.png\"),\n",
    "                path.join(image_path, \"img/mountain_near-cliff2.png\"),\n",
    "            ]\n",
    "            self.near_cliff_img = [\n",
    "                pygame.transform.scale(pygame.image.load(f_name), self.cell_size)\n",
    "                for f_name in near_cliff_imgs\n",
    "            ]\n",
    "        if self.cliff_img is None:\n",
    "            file_name = path.join(image_path, \"img/mountain_cliff.png\")\n",
    "            self.cliff_img = pygame.transform.scale(\n",
    "                pygame.image.load(file_name), self.cell_size\n",
    "            )\n",
    "\n",
    "        for s in range(self.nS):\n",
    "            row, col = np.unravel_index(s, self.shape)\n",
    "            pos = (col * self.cell_size[0], row * self.cell_size[1])\n",
    "            check_board_mask = row % 2 ^ col % 2\n",
    "            self.window_surface.blit(self.mountain_bg_img[check_board_mask], pos)\n",
    "\n",
    "            if self._cliff[row, col]:\n",
    "                self.window_surface.blit(self.cliff_img, pos)\n",
    "            if s == self.start_state_index:\n",
    "                self.window_surface.blit(self.start_img, pos)\n",
    "            if s == self.nS - 1:\n",
    "                self.window_surface.blit(self.goal_img, pos)\n",
    "            if s == self.s:\n",
    "                elf_pos = (pos[0], pos[1] - 0.1 * self.cell_size[1])\n",
    "                last_action = self.lastaction if self.lastaction is not None else 2\n",
    "                self.window_surface.blit(self.elf_images[last_action], elf_pos)\n",
    "\n",
    "        if mode == \"human\":\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:  # rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(self.window_surface)), axes=(1, 0, 2)\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73034518-ea7d-45cb-8e6e-4ffb2d4e51fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an environment\n",
    "def main():\n",
    "    env = CliffWalking(render_mode=\"human\")\n",
    "    observation, info = env.reset(seed=30)\n",
    "    enviroment = enviroment_init()\n",
    "    values, policy = mdp_algorithm(enviroment, env)\n",
    "    # Define the maximum number of iterations\n",
    "    max_iter_number = 1000\n",
    "    next_state = 36\n",
    "    counter = 0\n",
    "    fail = 0\n",
    "    for i in range(len(policy)):\n",
    "        print(i, ':', policy[i], end='   ')\n",
    "    print('\\n\\n')\n",
    "    for i in range(len(policy)):\n",
    "        print(i, ':', values[i], end='   ')\n",
    "\n",
    "    for __ in range(max_iter_number):\n",
    "        # TODO: Implement the agent policy here\n",
    "        # Note: .sample() is used to sample random action from the environment's action space\n",
    "\n",
    "        # Choose an action (Replace this random action with your agent's policy)\n",
    "        action = policy[next_state]\n",
    "\n",
    "        # Perform the action and receive feedback from the environment\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        if next_state == 47:\n",
    "            counter += 1\n",
    "\n",
    "        if reward == -100:\n",
    "            fail += 1\n",
    "\n",
    "        # print(next_state)\n",
    "\n",
    "        if done or truncated:\n",
    "            observation, info = env.reset()\n",
    "\n",
    "    # Close the environment\n",
    "    print(counter)\n",
    "    print(fail)\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a151df65-472f-4240-bbf4-8ebbb26f9ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enviroment():\n",
    "    enviroment = {}\n",
    "    for state in range(47):\n",
    "        c = state % 12\n",
    "        enviroment[state] = {}\n",
    "        for action in possible_actions(state):\n",
    "            enviroment[state][action] = []\n",
    "            h = actions(state, action)\n",
    "            for i in range(len(h)):\n",
    "                reward = (-1 / (10 * (c + 1))) ** 3\n",
    "                enviroment[state][action].append((1 / 3, reward, next_state(state, h[i])))\n",
    "\n",
    "    return enviroment\n",
    "\n",
    "\n",
    "def actions(s, a):\n",
    "    r = s // 12\n",
    "    c = s % 12\n",
    "\n",
    "    action = []\n",
    "    if a == 0:\n",
    "        action.append(0)\n",
    "        if c != 0:\n",
    "            action.append(3)\n",
    "        else:\n",
    "            action.append(-1)\n",
    "\n",
    "        if c != 11:\n",
    "            action.append(1)\n",
    "        else:\n",
    "            action.append(-1)\n",
    "\n",
    "    if a == 1:\n",
    "        action.append(1)\n",
    "        if r != 0:\n",
    "            action.append(0)\n",
    "        else:\n",
    "            action.append(-1)\n",
    "\n",
    "        if r != 3:\n",
    "            action.append(2)\n",
    "        else:\n",
    "            action.append(-1)\n",
    "\n",
    "    if a == 2:\n",
    "        action.append(2)\n",
    "        if c != 0:\n",
    "            action.append(3)\n",
    "        else:\n",
    "            action.append(-1)\n",
    "\n",
    "        if c != 11:\n",
    "            action.append(1)\n",
    "        else:\n",
    "            action.append(-1)\n",
    "\n",
    "    if a == 3:\n",
    "        action.append(3)\n",
    "        if r != 0:\n",
    "            action.append(0)\n",
    "        else:\n",
    "            action.append(-1)\n",
    "\n",
    "        if r != 3:\n",
    "            action.append(2)\n",
    "        else:\n",
    "            action.append(-1)\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "def possible_actions(s):\n",
    "    r = s // 12\n",
    "    c = s % 12\n",
    "    action = []\n",
    "\n",
    "    if r != 0:\n",
    "        action.append(0)\n",
    "    if r != 3:\n",
    "        action.append(2)\n",
    "    if c != 0:\n",
    "        action.append(3)\n",
    "    if c != 11:\n",
    "        action.append(1)\n",
    "\n",
    "    return action\n",
    "\n",
    "def next_state(s, a):\n",
    "    if a == 0:\n",
    "        return s - 12\n",
    "    if a == 1:\n",
    "        return s + 1\n",
    "    if a == 2:\n",
    "        return s + 12\n",
    "    if a == 3:\n",
    "        return s - 1\n",
    "    if a == -1:\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754d0312-fccc-441d-8ff4-9ed6d3183046",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
